{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayur619/NMT/blob/master/nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3751KCdDf3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfwb7m-ZIkzx",
        "colab_type": "code",
        "outputId": "b8574def-e616-4f81-b1ef-d3ae9d061d7c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 96
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-312d55b2-f3d3-47b2-84da-ad5ddc2a0792\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-312d55b2-f3d3-47b2-84da-ad5ddc2a0792\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving deu.txt to deu.txt\n",
            "User uploaded file \"deu.txt\" with length 12816559 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKU0osDlPX26",
        "colab_type": "code",
        "outputId": "a21e8355-bc1b-481a-fe04-7b09d061300b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!curl https://nlp.stanford.edu/data/glove.6B.zip --output glove.zip\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  822M  100  822M    0     0  24.5M      0  0:00:33  0:00:33 --:--:-- 31.3M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwMeBenNP0XZ",
        "colab_type": "code",
        "outputId": "94f0d6ee-bd80-4f2f-a9f3-8ea1b4afc2c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!unzip glove.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpbk0PG0QOUb",
        "colab_type": "code",
        "outputId": "31dc7e0f-62de-46e8-8bf1-04253963b419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3927
        }
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM,Dense,Input,Embedding\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "EPOCHS=100\n",
        "BATCH_SIZE=64\n",
        "MAX_VOCAB_SIZE=20000\n",
        "NUM_SAMPLES=10000\n",
        "EMBEDDING_DIM=100\n",
        "LATENT_DIM=256\n",
        "MAX_SEQ_LEN=100\n",
        "\n",
        "t=0\n",
        "input_texts=[]\n",
        "target_texts=[]\n",
        "target_texts_input=[]\n",
        "for line in open('deu.txt','r'):\n",
        "    if t>NUM_SAMPLES:\n",
        "        break\n",
        "    if '\\t' not in line:\n",
        "        continue\n",
        "    input_text,translation=line.rstrip().split('\\t')\n",
        "    target_text=translation+' <eos>'\n",
        "    target_text_input='<sos> '+translation\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    target_texts_input.append(target_text_input)\n",
        "    t+=1\n",
        "\n",
        "tokenizer_inputs=Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences=tokenizer_inputs.texts_to_sequences(input_texts)\n",
        "\n",
        "tokenizer_outputs=Tokenizer(num_words=MAX_VOCAB_SIZE,filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_texts+target_texts_input)\n",
        "targets_sequences=tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "targets_text_inputs_sequences=tokenizer_outputs.texts_to_sequences(target_texts_input)\n",
        "\n",
        "word2idx_inputs=tokenizer_inputs.word_index\n",
        "print('Found %d unique input tokens'%(len(word2idx_inputs)))\n",
        "word2idx_outputs=tokenizer_outputs.word_index\n",
        "print('Found %d unique output tokens'%(len(word2idx_outputs)))\n",
        "\n",
        "max_len_input=max(len(s) for s in input_sequences)\n",
        "max_len_targets=max(len(s) for s in targets_sequences)\n",
        "\n",
        "num_words_outputs=len(word2idx_outputs)+1\n",
        "encoder_inputs=pad_sequences(input_sequences,maxlen=max_len_input,padding='post')\n",
        "decoder_inputs=pad_sequences(targets_text_inputs_sequences,maxlen=max_len_targets,padding='post')\n",
        "decoder_targets=pad_sequences(targets_sequences,maxlen=max_len_targets,padding='post')\n",
        "\n",
        "print('Loading word vectors....')\n",
        "word2vec={}\n",
        "for line in open('glove.6B.100d.txt'):\n",
        "    line=line.split()\n",
        "    word=line[0]\n",
        "    vec=np.asarray(line[1:],dtype=np.float32)\n",
        "    word2vec[word]=vec\n",
        "\n",
        "print('Applying pre-trained glove....')\n",
        "num_words=min(MAX_VOCAB_SIZE,len(word2idx_inputs)+1)\n",
        "embedding_matrix=np.zeros((num_words,EMBEDDING_DIM))\n",
        "for word,idx in word2idx_inputs.items():\n",
        "    if idx<MAX_VOCAB_SIZE:\n",
        "        vector=word2vec.get(word)\n",
        "        if vector is not None:\n",
        "            embedding_matrix[idx]=vector\n",
        "\n",
        "embedding_layer=Embedding(num_words,EMBEDDING_DIM,weights=[embedding_matrix],input_length=max_len_input)\n",
        "decoder_targets_one_hot=np.zeros((len(input_texts),max_len_targets,num_words_outputs))\n",
        "for i,target in enumerate(decoder_targets):\n",
        "    for t,word in enumerate(target):\n",
        "        decoder_targets_one_hot[i,t,word]=1\n",
        "\n",
        "encoder_input_placeholder=Input(shape=(max_len_input,))\n",
        "x=embedding_layer(encoder_input_placeholder)\n",
        "encoder=LSTM(LATENT_DIM,return_state=True)\n",
        "encoder_out,h,c=encoder(x)\n",
        "encoder_state=[h,c]\n",
        "\n",
        "decoder_input_placeholder=Input(shape=(max_len_targets,))\n",
        "decoder_embedding=Embedding(num_words_outputs,LATENT_DIM)\n",
        "decoder_input_x=decoder_embedding(decoder_input_placeholder)\n",
        "decoder=LSTM(LATENT_DIM,return_sequences=True,return_state=True)\n",
        "decoder_out,_,_=decoder(decoder_input_x,initial_state=encoder_state)\n",
        "decoder_dense=Dense(num_words_outputs,activation='softmax')\n",
        "decoder_output=decoder_dense(decoder_out)\n",
        "seq2seq=Model([encoder_input_placeholder,decoder_input_placeholder],decoder_output)\n",
        "\n",
        "seq2seq.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "seq2seq.fit([encoder_inputs,decoder_inputs],decoder_targets_one_hot,batch_size=BATCH_SIZE,epochs=EPOCHS,validation_split=0.2)\n",
        "seq2seq.save('s2sg.h5')\n",
        "\n",
        "encoder_model=Model(encoder_input_placeholder,encoder_state)\n",
        "decoder_input_h=Input(shape=(LATENT_DIM,))\n",
        "decoder_input_c=Input(shape=(LATENT_DIM,))\n",
        "decoder_states_inputs=[decoder_input_h,decoder_input_c]\n",
        "decoder_input_single=Input(shape=(1,))\n",
        "decoder_input_single_x=decoder_embedding(decoder_input_single)\n",
        "decoder_outputs,h,c=decoder(decoder_input_single_x,initial_state=decoder_states_inputs)\n",
        "decoder_states=[h,c]\n",
        "decoder_outputs=decoder_dense(decoder_outputs)\n",
        "decoder_model=Model([decoder_input_single]+decoder_states_inputs,[decoder_outputs]+decoder_states)\n",
        "\n",
        "idx2word={v:k for k,v in word2idx_inputs.items()}\n",
        "idx2trans={v:k for k,v in word2idx_outputs.items()}\n",
        "\n",
        "def translate(input_seq):\n",
        "    states_value=encoder_model.predict(input_seq)\n",
        "    target_seq=np.zeros((1,1))\n",
        "    target_seq[0,0]=word2idx_outputs['<sos>']\n",
        "    eos=word2idx_outputs['<eos>']\n",
        "    output_sentence=[]\n",
        "    for _ in range(max_len_targets):\n",
        "        output_tokens,h,c=decoder_model.predict([target_seq]+states_value)\n",
        "        idx=np.argmax(output_tokens[0,0,:])\n",
        "        if eos==idx:\n",
        "            break\n",
        "        if idx>0:\n",
        "            output_sentence.append(idx2trans[idx])\n",
        "        target_seq[0,0]=idx\n",
        "        states_value=[h,c]\n",
        "    return ' '.join(output_sentence)\n",
        "\n",
        "while True:\n",
        "  input_=input('Input:')\n",
        "  translation = translate(pad_sequences(tokenizer_inputs.texts_to_sequences([input_]),maxlen=max_len_input,padding='post'))\n",
        "  print('Input:', input_)\n",
        "  print('Translation:', translation)\n",
        "\n",
        "  ans = input(\"Continue? [Y/n]\")\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 2332 unique input tokens\n",
            "Found 5207 unique output tokens\n",
            "Loading word vectors....\n",
            "Applying pre-trained glove....\n",
            "Train on 8000 samples, validate on 2001 samples\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 2.3176 - acc: 0.6853 - val_loss: 2.2745 - val_acc: 0.6868\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 1.6579 - acc: 0.7534 - val_loss: 2.0551 - val_acc: 0.7172\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 1.4563 - acc: 0.7783 - val_loss: 1.9302 - val_acc: 0.7322\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 1.3362 - acc: 0.7917 - val_loss: 1.8746 - val_acc: 0.7424\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 1.2378 - acc: 0.8034 - val_loss: 1.8065 - val_acc: 0.7517\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 1.1517 - acc: 0.8131 - val_loss: 1.7781 - val_acc: 0.7588\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 1.0750 - acc: 0.8218 - val_loss: 1.7388 - val_acc: 0.7641\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 1.0061 - acc: 0.8301 - val_loss: 1.7445 - val_acc: 0.7663\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.9457 - acc: 0.8373 - val_loss: 1.7213 - val_acc: 0.7700\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.8938 - acc: 0.8445 - val_loss: 1.7091 - val_acc: 0.7699\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.8460 - acc: 0.8515 - val_loss: 1.7035 - val_acc: 0.7707\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.8041 - acc: 0.8569 - val_loss: 1.7083 - val_acc: 0.7705\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.7608 - acc: 0.8644 - val_loss: 1.6928 - val_acc: 0.7746\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.7160 - acc: 0.8710 - val_loss: 1.7061 - val_acc: 0.7716\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.6755 - acc: 0.8770 - val_loss: 1.7018 - val_acc: 0.7701\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.6401 - acc: 0.8829 - val_loss: 1.7146 - val_acc: 0.7696\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.6076 - acc: 0.8888 - val_loss: 1.7165 - val_acc: 0.7701\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.5749 - acc: 0.8936 - val_loss: 1.7236 - val_acc: 0.7702\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.5442 - acc: 0.8994 - val_loss: 1.7420 - val_acc: 0.7672\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.5165 - acc: 0.9043 - val_loss: 1.7579 - val_acc: 0.7665\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4883 - acc: 0.9084 - val_loss: 1.7560 - val_acc: 0.7659\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4642 - acc: 0.9120 - val_loss: 1.7875 - val_acc: 0.7653\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4400 - acc: 0.9172 - val_loss: 1.7987 - val_acc: 0.7658\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4189 - acc: 0.9203 - val_loss: 1.8135 - val_acc: 0.7644\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3982 - acc: 0.9237 - val_loss: 1.8167 - val_acc: 0.7640\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3791 - acc: 0.9269 - val_loss: 1.8262 - val_acc: 0.7673\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3603 - acc: 0.9304 - val_loss: 1.8319 - val_acc: 0.7658\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3420 - acc: 0.9332 - val_loss: 1.8437 - val_acc: 0.7671\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3275 - acc: 0.9355 - val_loss: 1.8430 - val_acc: 0.7662\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3135 - acc: 0.9376 - val_loss: 1.8510 - val_acc: 0.7670\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3001 - acc: 0.9402 - val_loss: 1.8585 - val_acc: 0.7666\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2863 - acc: 0.9426 - val_loss: 1.8652 - val_acc: 0.7666\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2748 - acc: 0.9445 - val_loss: 1.8735 - val_acc: 0.7663\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2625 - acc: 0.9467 - val_loss: 1.8845 - val_acc: 0.7663\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2521 - acc: 0.9486 - val_loss: 1.8706 - val_acc: 0.7668\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2430 - acc: 0.9497 - val_loss: 1.8981 - val_acc: 0.7658\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2326 - acc: 0.9515 - val_loss: 1.9122 - val_acc: 0.7655\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2245 - acc: 0.9527 - val_loss: 1.9119 - val_acc: 0.7653\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2175 - acc: 0.9538 - val_loss: 1.9051 - val_acc: 0.7653\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2087 - acc: 0.9558 - val_loss: 1.9193 - val_acc: 0.7646\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2017 - acc: 0.9568 - val_loss: 1.9172 - val_acc: 0.7657\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1941 - acc: 0.9577 - val_loss: 1.9354 - val_acc: 0.7655\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1871 - acc: 0.9594 - val_loss: 1.9359 - val_acc: 0.7643\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1816 - acc: 0.9599 - val_loss: 1.9604 - val_acc: 0.7644\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1750 - acc: 0.9607 - val_loss: 1.9529 - val_acc: 0.7644\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1695 - acc: 0.9612 - val_loss: 1.9693 - val_acc: 0.7648\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1642 - acc: 0.9621 - val_loss: 1.9773 - val_acc: 0.7625\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1597 - acc: 0.9622 - val_loss: 1.9845 - val_acc: 0.7646\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1539 - acc: 0.9636 - val_loss: 1.9837 - val_acc: 0.7633\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1496 - acc: 0.9639 - val_loss: 1.9943 - val_acc: 0.7628\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1453 - acc: 0.9641 - val_loss: 2.0098 - val_acc: 0.7633\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1409 - acc: 0.9644 - val_loss: 2.0089 - val_acc: 0.7642\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1372 - acc: 0.9650 - val_loss: 2.0168 - val_acc: 0.7628\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1334 - acc: 0.9653 - val_loss: 2.0134 - val_acc: 0.7638\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1299 - acc: 0.9653 - val_loss: 2.0165 - val_acc: 0.7651\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1264 - acc: 0.9652 - val_loss: 2.0404 - val_acc: 0.7636\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1232 - acc: 0.9658 - val_loss: 2.0422 - val_acc: 0.7635\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1216 - acc: 0.9654 - val_loss: 2.0483 - val_acc: 0.7618\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1193 - acc: 0.9653 - val_loss: 2.0420 - val_acc: 0.7628\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1170 - acc: 0.9656 - val_loss: 2.0496 - val_acc: 0.7638\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1146 - acc: 0.9657 - val_loss: 2.0600 - val_acc: 0.7638\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1128 - acc: 0.9663 - val_loss: 2.0725 - val_acc: 0.7637\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1112 - acc: 0.9655 - val_loss: 2.0755 - val_acc: 0.7643\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1098 - acc: 0.9659 - val_loss: 2.0905 - val_acc: 0.7633\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1081 - acc: 0.9661 - val_loss: 2.0896 - val_acc: 0.7626\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1065 - acc: 0.9662 - val_loss: 2.0887 - val_acc: 0.7639\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1048 - acc: 0.9662 - val_loss: 2.1050 - val_acc: 0.7620\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1034 - acc: 0.9662 - val_loss: 2.1040 - val_acc: 0.7633\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1025 - acc: 0.9660 - val_loss: 2.1068 - val_acc: 0.7643\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1013 - acc: 0.9659 - val_loss: 2.1212 - val_acc: 0.7639\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0997 - acc: 0.9659 - val_loss: 2.1182 - val_acc: 0.7635\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0990 - acc: 0.9662 - val_loss: 2.1282 - val_acc: 0.7638\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0980 - acc: 0.9662 - val_loss: 2.1265 - val_acc: 0.7638\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0966 - acc: 0.9657 - val_loss: 2.1351 - val_acc: 0.7637\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0958 - acc: 0.9661 - val_loss: 2.1324 - val_acc: 0.7634\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0948 - acc: 0.9656 - val_loss: 2.1428 - val_acc: 0.7623\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0940 - acc: 0.9663 - val_loss: 2.1480 - val_acc: 0.7630\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0929 - acc: 0.9657 - val_loss: 2.1532 - val_acc: 0.7622\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0924 - acc: 0.9656 - val_loss: 2.1491 - val_acc: 0.7621\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0918 - acc: 0.9659 - val_loss: 2.1435 - val_acc: 0.7625\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0906 - acc: 0.9659 - val_loss: 2.1465 - val_acc: 0.7644\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0905 - acc: 0.9658 - val_loss: 2.1478 - val_acc: 0.7621\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0897 - acc: 0.9658 - val_loss: 2.1523 - val_acc: 0.7633\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0886 - acc: 0.9660 - val_loss: 2.1708 - val_acc: 0.7629\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0884 - acc: 0.9661 - val_loss: 2.1826 - val_acc: 0.7628\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0873 - acc: 0.9657 - val_loss: 2.1673 - val_acc: 0.7623\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0873 - acc: 0.9659 - val_loss: 2.1725 - val_acc: 0.7625\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0868 - acc: 0.9659 - val_loss: 2.1791 - val_acc: 0.7623\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0857 - acc: 0.9659 - val_loss: 2.1783 - val_acc: 0.7624\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0858 - acc: 0.9652 - val_loss: 2.1791 - val_acc: 0.7629\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0851 - acc: 0.9655 - val_loss: 2.1912 - val_acc: 0.7630\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0846 - acc: 0.9656 - val_loss: 2.1864 - val_acc: 0.7601\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0841 - acc: 0.9657 - val_loss: 2.1962 - val_acc: 0.7636\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0836 - acc: 0.9660 - val_loss: 2.1913 - val_acc: 0.7632\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0829 - acc: 0.9657 - val_loss: 2.2025 - val_acc: 0.7624\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0829 - acc: 0.9652 - val_loss: 2.1933 - val_acc: 0.7633\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0822 - acc: 0.9658 - val_loss: 2.2065 - val_acc: 0.7623\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0821 - acc: 0.9650 - val_loss: 2.2017 - val_acc: 0.7623\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0814 - acc: 0.9658 - val_loss: 2.2256 - val_acc: 0.7634\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0807 - acc: 0.9652 - val_loss: 2.2280 - val_acc: 0.7625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input:Hi.\n",
            "Input: Hi.\n",
            "Translation: hallo!\n",
            "Continue? [Y/n]y\n",
            "Input:Bye.\n",
            "Input: Bye.\n",
            "Translation: stürmte karte.\n",
            "Continue? [Y/n]y\n",
            "Input:Hug me.\n",
            "Input: Hug me.\n",
            "Translation: drück mich!\n",
            "Continue? [Y/n]\n",
            "Input:you won.\n",
            "Input: you won.\n",
            "Translation: du hast gewonnen.\n",
            "Continue? [Y/n]n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RciJ_aSMuGeA",
        "colab_type": "code",
        "outputId": "a39b3018-fad2-4b7e-efd5-0cd0d107a825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3684
        }
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense,Input,LSTM,Embedding\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "EPOCHS=100\n",
        "BATCH_SIZE=128\n",
        "MAX_VOCAB_SIZE=20000\n",
        "NUM_SAMPLES=10000\n",
        "EMBEDDING_DIM=100\n",
        "LATENT_DIM=256\n",
        "MAX_SEQ_LEN=100\n",
        "\n",
        "t=0\n",
        "input_texts=[]\n",
        "target_texts=[]\n",
        "target_texts_input=[]\n",
        "for line in open('deu.txt','r'):\n",
        "    if t>NUM_SAMPLES:\n",
        "        break\n",
        "    if '\\t' not in line:\n",
        "        continue\n",
        "    input_text,translation=line.rstrip().split('\\t')\n",
        "    target_text=translation+' <eos>'\n",
        "    target_text_input='<sos> '+translation\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    target_texts_input.append(target_text_input)\n",
        "    t+=1\n",
        "\n",
        "tokenizer_inputs=Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences=tokenizer_inputs.texts_to_sequences(input_texts)\n",
        "\n",
        "tokenizer_outputs=Tokenizer(num_words=MAX_VOCAB_SIZE,filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_texts+target_texts_input)\n",
        "targets_sequences=tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "targets_text_inputs_sequences=tokenizer_outputs.texts_to_sequences(target_texts_input)\n",
        "\n",
        "word2idx_inputs=tokenizer_inputs.word_index\n",
        "print('Found %d unique input tokens'%(len(word2idx_inputs)))\n",
        "word2idx_outputs=tokenizer_outputs.word_index\n",
        "print('Found %d unique output tokens'%(len(word2idx_outputs)))\n",
        "\n",
        "max_len_input=max(len(s) for s in input_sequences)\n",
        "max_len_targets=max(len(s) for s in targets_sequences)\n",
        "\n",
        "num_words_outputs=len(word2idx_outputs)+1\n",
        "encoder_inputs=pad_sequences(input_sequences,maxlen=max_len_input,padding='post')\n",
        "decoder_inputs=pad_sequences(targets_text_inputs_sequences,maxlen=max_len_targets,padding='post')\n",
        "decoder_targets=pad_sequences(targets_sequences,maxlen=max_len_targets,padding='post')\n",
        "\n",
        "print('Loading word vectors....')\n",
        "word2vec={}\n",
        "for line in open('glove.6B.100d.txt'):\n",
        "    line=line.split()\n",
        "    word=line[0]\n",
        "    vec=np.asarray(line[1:],dtype=np.float32)\n",
        "    word2vec[word]=vec\n",
        "\n",
        "print('Applying pre-trained glove....')\n",
        "num_words=min(MAX_VOCAB_SIZE,len(word2idx_inputs)+1)\n",
        "embedding_matrix=np.zeros((num_words,EMBEDDING_DIM))\n",
        "for word,idx in word2idx_inputs.items():\n",
        "    if idx<MAX_VOCAB_SIZE:\n",
        "        vector=word2vec.get(word)\n",
        "        if vector is not None:\n",
        "            embedding_matrix[idx]=vector\n",
        "\n",
        "word2vec={}\n",
        "embedding_layer=Embedding(num_words,EMBEDDING_DIM,weights=[embedding_matrix],input_length=max_len_input)\n",
        "decoder_targets_one_hot=np.zeros((len(input_texts),max_len_targets,num_words_outputs))\n",
        "for i,target in enumerate(decoder_targets):\n",
        "    for t,word in enumerate(target):\n",
        "        decoder_targets_one_hot[i,t,word]=1\n",
        "\n",
        "encoder_input_placeholder=Input(shape=(max_len_input,))\n",
        "x=embedding_layer(encoder_input_placeholder)\n",
        "encoder_lstm_1=LSTM(LATENT_DIM,return_sequences=True)(x)\n",
        "encoder_lstm_2=LSTM(LATENT_DIM,return_sequences=True,return_state=True)\n",
        "encoder_out,h,c=encoder_lstm_2(encoder_lstm_1)\n",
        "encoder_state=[h,c]\n",
        "\n",
        "decoder_input_placeholder=Input(shape=(max_len_targets,))\n",
        "decoder_embedding=Embedding(num_words_outputs,LATENT_DIM)\n",
        "decoder_input_x=decoder_embedding(decoder_input_placeholder)\n",
        "decoder_lstm_1=LSTM(LATENT_DIM,return_sequences=True)(decoder_input_x)\n",
        "decoder=LSTM(LATENT_DIM,return_state=True,return_sequences=True)\n",
        "decoder_out,_,_=decoder(decoder_lstm_1,initial_state=encoder_state)\n",
        "decoder_dense=Dense(num_words_outputs,activation='softmax')\n",
        "decoder_output=decoder_dense(decoder_out)\n",
        "seq2seq=Model([encoder_input_placeholder,decoder_input_placeholder],decoder_output)\n",
        "#seq2seq.load_weights('s2sg_weights.h5')\n",
        "seq2seq.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "seq2seq.fit([encoder_inputs,decoder_inputs],decoder_targets_one_hot,batch_size=BATCH_SIZE,epochs=EPOCHS,validation_split=0.2)\n",
        "#seq2seq.save('s2sg.h5')\n",
        "\n",
        "encoder_model=Model(encoder_input_placeholder,encoder_state)\n",
        "decoder_input_h=Input(shape=(LATENT_DIM,))\n",
        "decoder_input_c=Input(shape=(LATENT_DIM,))\n",
        "decoder_states_inputs=[decoder_input_h,decoder_input_c]\n",
        "decoder_input_single=Input(shape=(1,))\n",
        "decoder_input_single_x=decoder_embedding(decoder_input_single)\n",
        "decoder_outputs,h,c=decoder(decoder_input_single_x,initial_state=decoder_states_inputs)\n",
        "decoder_states=[h,c]\n",
        "decoder_outputs=decoder_dense(decoder_outputs)\n",
        "decoder_model=Model([decoder_input_single]+decoder_states_inputs,[decoder_outputs]+decoder_states)\n",
        "\n",
        "idx2word={v:k for k,v in word2idx_inputs.items()}\n",
        "idx2trans={v:k for k,v in word2idx_outputs.items()}\n",
        "\n",
        "def translate(input_seq):\n",
        "    states_value=encoder_model.predict(input_seq)\n",
        "    target_seq=np.zeros((1,1))\n",
        "    target_seq[0,0]=word2idx_outputs['<sos>']\n",
        "    eos=word2idx_outputs['<eos>']\n",
        "    output_sentence=[]\n",
        "    for _ in range(max_len_targets):\n",
        "        output_tokens,h,c=decoder_model.predict([target_seq]+states_value)\n",
        "        idx=np.argmax(output_tokens[0,0,:])\n",
        "        if eos==idx:\n",
        "            break\n",
        "        if idx>0:\n",
        "            output_sentence.append(idx2trans[idx])\n",
        "        target_seq[0,0]=idx\n",
        "        states_value=[h,c]\n",
        "    return ' '.join(output_sentence)\n",
        "\n",
        "while True:\n",
        "  input_=input('Input:')\n",
        "  translation = translate(pad_sequences(tokenizer_inputs.texts_to_sequences([input_]),maxlen=max_len_input,padding='post'))\n",
        "  print('Input:', input_)\n",
        "  print('Translation:', translation)\n",
        "\n",
        "  ans = input(\"Continue? [Y/n]\")\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 2332 unique input tokens\n",
            "Found 5207 unique output tokens\n",
            "Loading word vectors....\n",
            "Applying pre-trained glove....\n",
            "Train on 8000 samples, validate on 2001 samples\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 2.5984 - acc: 0.6459 - val_loss: 2.4635 - val_acc: 0.6684\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.8939 - acc: 0.7237 - val_loss: 2.2953 - val_acc: 0.6887\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.6826 - acc: 0.7432 - val_loss: 2.1851 - val_acc: 0.6913\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.5297 - acc: 0.7574 - val_loss: 2.1032 - val_acc: 0.7091\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.4142 - acc: 0.7728 - val_loss: 2.0599 - val_acc: 0.7230\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.3238 - acc: 0.7842 - val_loss: 2.0499 - val_acc: 0.7146\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 1.2496 - acc: 0.7925 - val_loss: 2.0024 - val_acc: 0.7240\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 1.1828 - acc: 0.8000 - val_loss: 1.9647 - val_acc: 0.7343\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 1.1234 - acc: 0.8065 - val_loss: 1.9787 - val_acc: 0.7267\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 1.0667 - acc: 0.8122 - val_loss: 1.9947 - val_acc: 0.7235\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.0134 - acc: 0.8177 - val_loss: 1.9774 - val_acc: 0.7268\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.9631 - acc: 0.8238 - val_loss: 2.0333 - val_acc: 0.7171\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.9167 - acc: 0.8286 - val_loss: 2.0068 - val_acc: 0.7227\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.8750 - acc: 0.8341 - val_loss: 2.0512 - val_acc: 0.7194\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.8344 - acc: 0.8391 - val_loss: 2.0542 - val_acc: 0.7226\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7946 - acc: 0.8435 - val_loss: 2.1782 - val_acc: 0.7162\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7571 - acc: 0.8489 - val_loss: 2.1079 - val_acc: 0.7197\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.7231 - acc: 0.8541 - val_loss: 2.2556 - val_acc: 0.7177\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.6925 - acc: 0.8583 - val_loss: 2.3065 - val_acc: 0.7181\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.6611 - acc: 0.8633 - val_loss: 2.3005 - val_acc: 0.7176\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.6305 - acc: 0.8677 - val_loss: 2.3403 - val_acc: 0.7196\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5998 - acc: 0.8729 - val_loss: 2.3369 - val_acc: 0.7200\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5724 - acc: 0.8774 - val_loss: 2.3544 - val_acc: 0.7213\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5444 - acc: 0.8819 - val_loss: 2.3449 - val_acc: 0.7235\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5192 - acc: 0.8869 - val_loss: 2.2991 - val_acc: 0.7225\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4935 - acc: 0.8913 - val_loss: 2.3509 - val_acc: 0.7249\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4683 - acc: 0.8963 - val_loss: 2.3438 - val_acc: 0.7238\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4450 - acc: 0.9006 - val_loss: 2.3558 - val_acc: 0.7238\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4204 - acc: 0.9058 - val_loss: 2.3385 - val_acc: 0.7234\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3998 - acc: 0.9099 - val_loss: 2.3174 - val_acc: 0.7255\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3768 - acc: 0.9149 - val_loss: 2.3716 - val_acc: 0.7244\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3568 - acc: 0.9189 - val_loss: 2.3449 - val_acc: 0.7259\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3379 - acc: 0.9226 - val_loss: 2.3432 - val_acc: 0.7282\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3191 - acc: 0.9264 - val_loss: 2.3506 - val_acc: 0.7311\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3022 - acc: 0.9305 - val_loss: 2.3612 - val_acc: 0.7264\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2857 - acc: 0.9340 - val_loss: 2.3935 - val_acc: 0.7280\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2676 - acc: 0.9374 - val_loss: 2.3644 - val_acc: 0.7283\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2526 - acc: 0.9406 - val_loss: 2.4101 - val_acc: 0.7250\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2374 - acc: 0.9441 - val_loss: 2.3824 - val_acc: 0.7287\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2254 - acc: 0.9460 - val_loss: 2.4179 - val_acc: 0.7263\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2128 - acc: 0.9486 - val_loss: 2.4484 - val_acc: 0.7252\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2010 - acc: 0.9504 - val_loss: 2.4161 - val_acc: 0.7271\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1908 - acc: 0.9535 - val_loss: 2.4313 - val_acc: 0.7275\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1801 - acc: 0.9553 - val_loss: 2.4177 - val_acc: 0.7287\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1717 - acc: 0.9562 - val_loss: 2.4371 - val_acc: 0.7267\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1626 - acc: 0.9577 - val_loss: 2.4188 - val_acc: 0.7274\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1546 - acc: 0.9594 - val_loss: 2.4343 - val_acc: 0.7288\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1477 - acc: 0.9601 - val_loss: 2.4486 - val_acc: 0.7266\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1403 - acc: 0.9610 - val_loss: 2.5391 - val_acc: 0.7227\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1341 - acc: 0.9621 - val_loss: 2.4638 - val_acc: 0.7249\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1292 - acc: 0.9626 - val_loss: 2.4902 - val_acc: 0.7274\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1232 - acc: 0.9631 - val_loss: 2.5168 - val_acc: 0.7226\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1191 - acc: 0.9631 - val_loss: 2.5004 - val_acc: 0.7272\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1152 - acc: 0.9640 - val_loss: 2.5699 - val_acc: 0.7200\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1113 - acc: 0.9638 - val_loss: 2.6434 - val_acc: 0.7189\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1071 - acc: 0.9645 - val_loss: 2.5473 - val_acc: 0.7252\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1046 - acc: 0.9645 - val_loss: 2.5591 - val_acc: 0.7251\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1015 - acc: 0.9651 - val_loss: 2.5521 - val_acc: 0.7244\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0986 - acc: 0.9654 - val_loss: 2.5768 - val_acc: 0.7242\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0966 - acc: 0.9655 - val_loss: 2.5677 - val_acc: 0.7241\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0935 - acc: 0.9658 - val_loss: 2.5743 - val_acc: 0.7262\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0919 - acc: 0.9652 - val_loss: 2.5672 - val_acc: 0.7250\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0900 - acc: 0.9656 - val_loss: 2.5684 - val_acc: 0.7251\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0880 - acc: 0.9658 - val_loss: 2.5736 - val_acc: 0.7250\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0867 - acc: 0.9653 - val_loss: 2.6247 - val_acc: 0.7217\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0853 - acc: 0.9655 - val_loss: 2.6074 - val_acc: 0.7221\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0843 - acc: 0.9652 - val_loss: 2.6218 - val_acc: 0.7261\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0827 - acc: 0.9655 - val_loss: 2.6181 - val_acc: 0.7251\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0817 - acc: 0.9658 - val_loss: 2.6357 - val_acc: 0.7258\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0808 - acc: 0.9658 - val_loss: 2.6100 - val_acc: 0.7267\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0802 - acc: 0.9656 - val_loss: 2.6564 - val_acc: 0.7231\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0793 - acc: 0.9651 - val_loss: 2.6139 - val_acc: 0.7254\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0782 - acc: 0.9655 - val_loss: 2.5969 - val_acc: 0.7282\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.0778 - acc: 0.9652 - val_loss: 2.6720 - val_acc: 0.7233\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.0771 - acc: 0.9655 - val_loss: 2.6079 - val_acc: 0.7295\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0763 - acc: 0.9653 - val_loss: 2.6517 - val_acc: 0.7262\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0754 - acc: 0.9658 - val_loss: 2.6339 - val_acc: 0.7259\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0754 - acc: 0.9655 - val_loss: 2.6304 - val_acc: 0.7280\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0748 - acc: 0.9656 - val_loss: 2.6292 - val_acc: 0.7270\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0741 - acc: 0.9661 - val_loss: 2.6199 - val_acc: 0.7278\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0738 - acc: 0.9655 - val_loss: 2.6609 - val_acc: 0.7264\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0735 - acc: 0.9652 - val_loss: 2.6351 - val_acc: 0.7277\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.0730 - acc: 0.9657 - val_loss: 2.6616 - val_acc: 0.7261\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0724 - acc: 0.9655 - val_loss: 2.6785 - val_acc: 0.7269\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0722 - acc: 0.9654 - val_loss: 2.6865 - val_acc: 0.7268\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0722 - acc: 0.9652 - val_loss: 2.6477 - val_acc: 0.7281\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0715 - acc: 0.9654 - val_loss: 2.6903 - val_acc: 0.7266\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0710 - acc: 0.9655 - val_loss: 2.6731 - val_acc: 0.7284\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0708 - acc: 0.9656 - val_loss: 2.6495 - val_acc: 0.7302\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0706 - acc: 0.9652 - val_loss: 2.6920 - val_acc: 0.7271\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0703 - acc: 0.9654 - val_loss: 2.7179 - val_acc: 0.7265\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0698 - acc: 0.9659 - val_loss: 2.6614 - val_acc: 0.7272\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0697 - acc: 0.9651 - val_loss: 2.6807 - val_acc: 0.7280\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0697 - acc: 0.9651 - val_loss: 2.6725 - val_acc: 0.7285\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0693 - acc: 0.9655 - val_loss: 2.6877 - val_acc: 0.7286\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0693 - acc: 0.9653 - val_loss: 2.7200 - val_acc: 0.7261\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0692 - acc: 0.9650 - val_loss: 2.7006 - val_acc: 0.7283\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0683 - acc: 0.9655 - val_loss: 2.6903 - val_acc: 0.7291\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0689 - acc: 0.9652 - val_loss: 2.7052 - val_acc: 0.7270\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.0683 - acc: 0.9654 - val_loss: 2.7409 - val_acc: 0.7269\n",
            "Input:Hi.\n",
            "Input: Hi.\n",
            "Translation: hallo! auf »tom« darüber ruhe! ruhe! fort! fort! fort! fort! zuhause.\n",
            "Continue? [Y/n]n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFKhsvuUWKP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq2seq.save_weights('s2sg_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcfl-1QZWK5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('s2sg_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}